---
title: UTF-8和Unicode关系
date: 2023-08-07
categories:
  - 计算机基础
tags:
  - 编码
---

## ASCII

起初，计算机是由美国人发明出来的，所以美国人为了将自己的语言也就是英语转换成计算机所识别二进制语言，于是乎呢，就自己编写一套编码，也就是ASCII码。

定义： 通过 1 字节的数据表示 256 个字符。描述英语中的字符和这 8 位二进制数的对应关系，这被称为 ASCII 码。

ASCII 码一共定义了 128 个字符，这 128 个字符只使用了 8 位二进制数中的后面 7 位，最前面的一位统一规定为 0。

## Unicode

随着计算机的发展，各个国家都发展计算机行业，用ASCII码来表示各个国家的语言是远远不够的。
于是，美国人就提出一种标准方案来展示世界上所有语言中的所有字符，出于这个目的，Unicode诞生了。

简单的说，Unicode 就是一本超级超级厚的字典，记录着世界上所有字符对应的一个数字，规定了符合对应的二进制代码，Unicode
给所有的字符指定了一个数字用来表示该字符。

> ASCII 和 Unicode 都可以看作是字典，不过ASCII是给美国用的字典，Unicode是给世界用的字典。并且 Unicode可以完全兼容ASCII。

对于软件开发调试时，对于低端机器打印中文传输结果时，会显示 `\uxxxx...`
等这类格式的字符串，究其原因是这类低端机器不存在Unicode字符集，所以无法转换为 `\uxxxx` unicode 对应的文字，
最终用 `ascii` 字符集来表示。

:::tip

对于编码，我们需要从三个方面来看：

1. 数据展示层面：对于 `Unicode` 而言，展示的内容其实就是 <span class="red">`Unicode` 的编号数字</span>； `你`
   这个字符，如果用 `char` 类型的格式展示，就是`你`, 如果用 `int` 类型的格式展示，就是`20319`;
2. 数据存储层面：由于`Unicode` 单字符取值空间比较大(0x0000 - 0x10FFFF),
   导致存储时有的字符占用1个字节，有的占用2个字节，甚至更多，当多个字节混在一起，我们不能区分究竟某几个字节表示一个字符还是多个字符，
   因此我们不能直接存储`Unicode` 的编号数字， 需要一种利于区分每个字符边界的格式来存储， 因此就需要对 `Unicode` 的编号数字
   进行编码。包含固定长度编码 `UTF-32`， 和非固定长度编码 `UTF-8`，`UTF-16`
3. 数据传输层面：传输层同样有存储层面的问题，但还有该层特有的端传输问题。及究竟是先传输低字节还是先传输高字节。网络传输规定一般采用大端存储：低字节存储高位，高字节存储低位。并且无论大端还是小端，发送顺序都是从低地址发送。

eg: 对于Java而言，一个字符占用2个字节，究其原因是因为 `Java内存` 中存储数据采用 `UTF-16`
格式，该格式对于基本面字符，就占用2个字节，对于多面字符，由于超过了 `uxxxx` 2个字节可以表示的极限，因此会用4个字节存储。
展示时，就是`Unicode`
的编号数字，当我们需要将内存的数据写入到文件中，其实就是将一种存储格式的数据写入到另外一种存储格式的文件中。对于java而言，就是将 `UTF-8`
写入到 `UTF-16` 中，
系统层面就需要将其内存中的 `UTF-16` 编码的字节数组 转换为 `UTF-8` 的字节数据。
:::

## UTF-8

Unicode中有的字符是占用多个字节，而计算机是不能区分一个字符占用的字节数，则需要使用一种方式分隔字符。如果使用定长字节数分割，则对于美国人而言，他们大量的文字都是占用一个字节，
却需要用三个字节传输，极大的浪费传输效率。因此诞生了UTF-8编码，最大特点就是可变长的。

1. 对于单个字节的字符，第一位设为 0，后面的 7 位对应这个字符的 Unicode 码点。因此，对于英文中的 0 - 127 号字符，与 ASCII
   码完全相同。
2. 对于多个字节的字符。 对于需要使用 N 个字节来表示的字符（N > 1），第一个字节的前 N 位都设为 1，第 N + 1 位设为0，剩余的 N -
   1 个字节的前两位都设位 10，
   剩下的二进制位则使用这个字符的 Unicode 码点来填充。不够的位用0来填充。

| Unicode十六进制码点范围           | UTF-8二进制                                | 十进制范围                        |
|---------------------------|-----------------------------------------|------------------------------|
| 0000  0000  -  0000  007F | 0xxxxxxx                                | 0  ~  127           （2^7）    |
| 0000  0080  -  0000  07FF | 110xxxxx  10xxxxxx                      | 128  ~  2047          (2^11) |
| 0000  0800  -  0000  FFFF | 1110xxxx 10xxxxxx 10xxxxxx              | 2048  ~  65535      (2^16)   |
| 0001  0000  -  0010  FFFF | 11110xxx   10xxxxxx  10xxxxxx  10xxxxxx | 65536  ~  2097151     (2^21) |

使用python对字符 '你' 进行编码转换

```python
# 在unicode十进制范围内为3字节表示   
ord('你')
# 结果： 20320

# 20320转化为十六进制 
hex(20320)
# 结果： 0x4f60

# 20320转化为二进制  
bin(20320)
# 结果： 0b100111101100000
 
# 1001 111011 00000 将二进制插入到 utf-8 的编码规则
# 结果： 11101001 10111011 1000000   

# 将 11101001 10111011 1000000 表示为 16进制
hex(0b11101001101110111000000)
# 结果： 0x74ddc0


hex(ord('你'))
# 结果： 0x4f60

'你'.encode('utf-8')
# 结果： b'\xe4\xbd\xa0'
```

## 总结

ASCII和Unicode属于编码的范畴，可以理解为字典。UTF-8属于编码方式的范畴，可以理解为加密（类似base64，虽然没有安全性可言）。
